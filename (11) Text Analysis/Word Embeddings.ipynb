{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66681b32",
   "metadata": {},
   "source": [
    "# Exploring Spacy with Embeddings!\n",
    "\n",
    "There are a lot of explanation about embeddings so I will not even try to explain it. What I am going to do here is to use embeddings to find words that are similar to each other. \n",
    "This similarrity means that they are use in the same context. So we are going to test this in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c74feecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install spacy\n",
    "# the line below should be use in the anaconda prompt selecting the correct environment \n",
    "# pip install en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b6decff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d71e81",
   "metadata": {},
   "source": [
    "## Embeddings on similar context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cd040a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.19343 ,  -4.1969  ,   4.8175  ,  -0.72863 ,   2.3177  ,\n",
       "        -1.4221  ,   0.28923 ,   0.062839,   3.6781  ,   2.6208  ,\n",
       "         1.8116  ,   0.42054 ,   3.3034  ,  -1.165   ,  -1.8362  ,\n",
       "        -2.4683  ,   4.2381  ,   1.2929  ,  -0.37599 ,   3.2744  ,\n",
       "        -2.8982  ,  -5.9219  ,  -1.8752  ,   3.8131  ,   6.583   ,\n",
       "        -0.16072 ,  -1.1781  ,  -2.7252  ,  -3.3267  ,  -0.16564 ,\n",
       "         1.4311  ,  -0.51942 ,   0.87652 ,   0.51414 ,   1.4174  ,\n",
       "        -1.4736  ,   1.8717  ,  -0.99453 ,  -6.5019  ,   1.6999  ,\n",
       "        -3.0466  ,  -2.4686  ,  -4.4889  ,   6.5907  ,   1.375   ,\n",
       "        -3.0183  ,  -4.4784  ,   2.7568  ,   4.5392  ,  -2.9311  ,\n",
       "        -3.6852  ,  -1.7053  ,   2.422   ,   3.9895  ,   5.0674  ,\n",
       "         1.3144  ,   1.0707  ,  -9.2608  ,   0.62933 ,   5.3289  ,\n",
       "        -3.6329  ,  -5.5805  ,   5.4988  ,   0.62285 ,   1.4319  ,\n",
       "         2.2446  ,  -1.9759  ,  -1.7883  ,   5.6889  ,  -6.1173  ,\n",
       "         0.40993 ,   1.436   ,  -6.6111  ,  -4.7627  ,  -1.945   ,\n",
       "         5.8846  ,  -5.4712  ,   6.2925  ,   2.2544  ,   2.4264  ,\n",
       "        -2.1848  ,  -0.63672 ,  -7.8964  ,   3.7743  ,  -1.7516  ,\n",
       "         1.0216  ,  -1.1627  ,   4.4604  ,  -5.9697  ,   0.2082  ,\n",
       "        -1.5752  ,  -3.1609  ,  -4.0534  ,  -1.889   ,  -6.2744  ,\n",
       "         0.86429 ,  -1.8791  ,  -0.42341 ,  -3.0094  ,   0.79509 ,\n",
       "        -4.2661  ,  -2.8265  ,  -0.85175 ,   1.7241  ,  -0.22838 ,\n",
       "         2.2058  ,  -1.6097  ,   1.1607  ,   6.4679  ,  -5.2997  ,\n",
       "        -6.2646  ,   5.0426  ,   3.4095  ,   3.4815  ,   9.2755  ,\n",
       "        -5.8113  ,  -4.5499  ,   0.50462 ,   1.5843  ,   2.2471  ,\n",
       "         1.5692  ,  -1.2785  ,   6.1438  ,  -8.9612  ,   4.1012  ,\n",
       "         2.6644  ,   0.6535  ,  -7.1344  ,   1.6875  ,  -0.32363 ,\n",
       "        -5.6092  ,   2.8563  ,   2.7854  ,  -0.71228 ,   0.43001 ,\n",
       "         0.87425 ,  -1.7621  ,   3.1546  ,  -4.5938  ,  -3.9465  ,\n",
       "        -6.2262  ,   0.33711 ,  -0.21528 ,  -3.8749  ,   2.2451  ,\n",
       "         0.2088  ,  -6.3412  ,   5.1258  ,  -3.0424  ,  -0.77623 ,\n",
       "         2.3595  ,   0.028703,  -1.0039  ,   1.9061  ,   1.4729  ,\n",
       "         2.5383  ,  -2.4957  ,  -2.2428  ,  -1.6663  ,   0.71359 ,\n",
       "         1.7846  , -11.062   ,  -3.1488  ,  -0.95036 ,  -7.5857  ,\n",
       "        -6.994   ,   0.37835 ,  -0.66691 ,  -0.49036 ,   0.92978 ,\n",
       "         6.4581  ,  -0.87936 ,   0.33482 ,  -1.6848  ,  -0.044265,\n",
       "        -0.11155 ,   1.3763  ,   1.0704  ,   0.4828  ,  -5.6289  ,\n",
       "         0.070978,  -0.90101 ,  -0.97243 ,   2.2958  ,   7.0401  ,\n",
       "         2.1671  ,  -0.031095,  -1.0682  ,  -3.3594  ,   3.0531  ,\n",
       "         2.2338  ,  -3.7636  ,   2.0585  ,   7.3505  ,  -0.42624 ,\n",
       "         6.3578  ,  -1.4151  ,   2.1862  ,   0.16724 ,   7.3919  ,\n",
       "         4.882   ,   5.1933  ,  -4.0285  ,   3.4392  ,  -1.7809  ,\n",
       "        -5.7629  ,   6.8964  ,   4.4143  ,  -7.3915  ,   5.6025  ,\n",
       "        -8.0512  ,  -0.3741  ,   3.6755  ,   7.5293  ,   0.44672 ,\n",
       "         0.33251 ,   1.8733  ,  -1.0936  ,   2.6618  ,  -8.3229  ,\n",
       "        -0.66927 ,   0.13974 ,  -4.4223  ,   1.3336  ,   2.7     ,\n",
       "        -1.5951  ,   0.83365 ,  11.612   ,  -5.5315  ,   0.94248 ,\n",
       "        -1.4752  ,  -0.2047  ,  -3.2312  ,   0.73125 ,   4.6758  ,\n",
       "         6.4229  ,  -3.0921  ,  -3.1441  ,  -9.691   ,   0.8753  ,\n",
       "         3.3832  ,   5.8956  ,  -4.9336  ,  -2.0511  ,  -1.2117  ,\n",
       "        -0.97065 ,   5.2081  ,   2.1007  ,   1.68    ,  -3.2119  ,\n",
       "        -0.059411,  -7.9966  ,  -1.1969  ,   1.6711  ,  -0.68285 ,\n",
       "        -8.9127  ,   9.2666  ,  -1.4378  ,   1.7714  ,   4.319   ,\n",
       "         0.19569 ,  -8.0474  ,  -2.2153  ,  -3.1312  ,  -0.58417 ,\n",
       "        -0.97893 ,  -1.1495  ,   0.22475 ,  -3.102   ,  -0.77219 ,\n",
       "        -5.7923  ,  -1.7915  ,  -4.7496  ,   2.1055  ,   1.2694  ,\n",
       "        -0.62912 ,   2.6512  ,  -0.52775 ,  -0.68296 ,   2.0287  ,\n",
       "        -7.5485  ,  -1.9485  ,   3.2463  ,   1.9657  ,   4.5191  ,\n",
       "        -1.4056  ,   1.6316  ,   2.8408  ,  -2.2202  ,  -2.9962  ,\n",
       "         1.3214  ,  -6.8758  ,  -1.8754  ,  -1.8526  ,   4.0454  ,\n",
       "         6.2775  ,   0.2443  ,  -1.8489  ,  -2.6881  ,   0.65135 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how an embedding looks like\n",
    "king = nlp('King').vector\n",
    "man = nlp('Man').vector\n",
    "woman = nlp('Woman').vector\n",
    "king"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "34688971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "King embedding shape: (300,)\n",
      "Man embedding shape: (300,)\n",
      "Woman embedding shape: (300,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"King embedding shape: {king.shape}\")\n",
    "print(f\"Man embedding shape: {man.shape}\") \n",
    "print(f\"Woman embedding shape: {woman.shape}\")\n",
    "# notice all have the same length!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f4f68c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so hopefully this substract give us the queen! \n",
    "queen = (king - man) + woman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1e4ead38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_words(vector):\n",
    "    queries = np.asarray([vector])\n",
    "    ms = nlp.vocab.vectors.most_similar(queries, n=10)\n",
    "    words = [nlp.vocab.strings[w] for w in ms[0][0]]\n",
    "    distances = ms[2]\n",
    "    print(words)\n",
    "    print(distances)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7f34c99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['King', '-King', 'KastKing', 'R.M.King', 'Kingi', 'King-', 'Kingz', 'Queen', 'Kingu', 'king']\n",
      "[[0.6952 0.6542 0.5835 0.5786 0.5623 0.5603 0.5439 0.5421 0.5272 0.5198]]\n"
     ]
    }
   ],
   "source": [
    "words = similar_words(queen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "78f548bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Man', 'Brak', 'Woman', 'Womad', 'Girl', 'Fist', '-Girl', 'Womanly', 'Boy', 'Fisto']\n",
      "[[1.     0.7302 0.6429 0.6036 0.5658 0.5608 0.5598 0.5513 0.542  0.5261]]\n"
     ]
    }
   ],
   "source": [
    "words = similar_words(man)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f601c2f7",
   "metadata": {},
   "source": [
    "# What are we seeing?\n",
    "\n",
    "So notice that we try to get Queen trying to substact the vector man from king and adding woman. \n",
    "but appearentely the vector king and queen are close together and the end result is that the most similar word at the end of all the operation was king. Notice that the word Man and woman are also close together!\n",
    "\n",
    "# comparing different vectors embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "594d290d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how an embedding looks like on synonymous\n",
    "dictionary = {}\n",
    "dictionary['happy'] = nlp('Happy').vector\n",
    "dictionary['cheerful'] = nlp('Cheerful').vector\n",
    "dictionary['delighted'] = nlp('Delighted').vector\n",
    "# antonymous\n",
    "dictionary['sad'] = nlp('Sad').vector\n",
    "# unrelated word\n",
    "dictionary['banana'] = nlp('Banana').vector\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5292fe69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Happy', '-Happy', 'Yappy', 'Happo', 'Happytime', 'appy', 'Dappy', 'Happn', 'Slappy', 'Sappy']\n",
      "[[1.     0.9015 0.7374 0.7176 0.7136 0.698  0.6947 0.6864 0.6795 0.6792]]\n"
     ]
    }
   ],
   "source": [
    "words = similar_words(dictionary['happy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "20e9f11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we are comparing happy embedding with happy\n",
      "1.0\n",
      "we are comparing happy embedding with cheerful\n",
      "0.50845826\n",
      "we are comparing happy embedding with delighted\n",
      "0.33883482\n",
      "we are comparing happy embedding with sad\n",
      "0.36764246\n",
      "we are comparing happy embedding with banana\n",
      "0.28835577\n"
     ]
    }
   ],
   "source": [
    "for elem in dictionary.keys():\n",
    "    print(f'we are comparing happy embedding with {elem}')\n",
    "    print(cosine_similarity(dictionary['happy'].reshape(1, -1),dictionary[elem].reshape(1, -1)).flatten()[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ee618c",
   "metadata": {},
   "source": [
    "## What are we seeing?\n",
    "Notice that the embeddings not always make the most sense. Sad is more similar to happy than delighted even though the latter is a synonymous. Maybe sad is more similar than delighted is because happy and sad are easier words and appear together for instance in children's book or whenever. So it is best **not to expect that similar words in meaning are similar in embedding** \n",
    "This is a way similar to T-sne, we think we understand the topic but the intuition fails because we expect that similar ideas or points in T-sne are group magically together, but they are not. So we need to use the concepts for what they are and what they truly provide us.\n",
    "\n",
    "** we will continue exploring this subject stayed tune***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e28a18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "pytorch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
