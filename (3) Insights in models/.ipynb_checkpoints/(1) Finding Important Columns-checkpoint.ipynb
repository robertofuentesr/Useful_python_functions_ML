{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef4aec49",
   "metadata": {},
   "source": [
    "I already created some machine learning models in this repo but I haven't say anything about what columns are important. That is a very important aspects that we need to address, when we understand what column are important we can:\n",
    "+ trust the result of the model\n",
    "+ take inform decision\n",
    "+ in case we need to collect new data (Generally expensive), we know what variables play a key role and thus we need to collect.\n",
    "\n",
    "So this is the purpose of this notebook and the next in this folder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d4d7247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91112138",
   "metadata": {},
   "source": [
    "### How we are going to train this data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10bc6bc",
   "metadata": {},
   "source": [
    "Previously we got the best model with n_estimators=369 when we use the **whole training data using cross validation**.\n",
    "We could improve the model even further but instead of doing that our focus is in getting the modest important variables.\n",
    "\n",
    "We are going to keep this hyperparameter but we are going to train a model with less data, because we are going to separate in the training set in training and validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d08b69b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "# This data you can find here: https://www.kaggle.com/c/home-data-for-ml-course/data\n",
    "\n",
    "X_full = pd.read_csv('train.csv', index_col='Id')\n",
    "\n",
    "# SalePrice is the target, if there is no target eliminate row associated with it\n",
    "X_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "y = X_full.SalePrice\n",
    "X = X_full.copy()\n",
    "X.drop(['SalePrice'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fcd8c9",
   "metadata": {},
   "source": [
    "The next cell just uses the same function that we saw in the \"(1) Predicting_a_numerical_value\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dacaef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_data(X,delete_over=10,col_to_change_to_null=0.1):\n",
    "    \n",
    "    categorical_variables = [col for col in  X.columns if str(X[col].dtypes)=='object']\n",
    "    #numerical_variables = [col for col in X.columns if str(X[col].dtypes)!='object']\n",
    "    cardinalidad = {}\n",
    "    for col in categorical_variables:\n",
    "        cardinalidad[col] = len(list(X[col].unique()))\n",
    "    # For now we delete categories with more values than..\n",
    "    delete_over = delete_over\n",
    "    columns_to_delete = [col for col in categorical_variables if len(list(X[col].unique()))>delete_over ]\n",
    "    X.drop(columns=columns_to_delete,inplace = True, axis=1)\n",
    "    \n",
    "    # We are going to change columns with too many null.\n",
    "    # We are not gonna delete them, will give them the chance to be important.\n",
    "    # that means that having or not having the value is what is really important.\n",
    "    col_to_change_to_null = col_to_change_to_null\n",
    "    columnas_modificar_por_1 = [col for col in X.columns if X[col].isnull().sum()>int(X.shape[0] * col_to_change_to_null) ]\n",
    "\n",
    "    for col in columnas_modificar_por_1:\n",
    "        X[col +str('_is_null')] = 0\n",
    "        X.loc[(X[col].isnull()),col +str('_is_null')] = 1\n",
    "\n",
    "    new_columns_null = [str(f\"{col}_is_null\") for col in columnas_modificar_por_1 ]    \n",
    "    X.drop(columns=columnas_modificar_por_1, axis=1,inplace=True)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76bf0a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cleaning_data(X)\n",
    "X, X_val, y, y_val = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
    "                                                random_state=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26f2d949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforming_modeling_scoring(X,y, n_estimators=369,n_jobs=-1):\n",
    "      \n",
    "    numerical_col = [col for col in X.columns if str(X[col].dtypes)!='object' ]\n",
    "    numerical_col_imputed = [col for col in numerical_col if X[col].isnull().any()==True]\n",
    "\n",
    "    categorical_col = [col for col in X.columns if str(X[col].dtypes)=='object' ]\n",
    "    categorical_col_imputed = [col for col in categorical_col if X[col].isnull().any()==True]\n",
    "\n",
    "    numerical_transformer = SimpleImputer(strategy='mean')\n",
    "\n",
    "    categorical_transformer =  Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(missing_values=pd.NA, strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(transformers=\n",
    "        [(\"numerical_transformer\", numerical_transformer, numerical_col_imputed),\n",
    "        (\"categorical_transformer\", categorical_transformer, categorical_col)],remainder='passthrough')\n",
    "\n",
    "    # Define model\n",
    "    model = RandomForestRegressor(n_estimators=n_estimators,random_state=0,n_jobs=n_jobs)\n",
    "\n",
    "    # Bundle preprocessing and modeling code in a pipeline\n",
    "    pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                          ('model', model)\n",
    "                         ])\n",
    "\n",
    "    model = pipe.fit(X,y)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8664cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transforming_modeling_scoring(X,y, n_estimators=369)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1ba2fb9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'if_delegate_has_method' from 'sklearn.utils.metaestimators' (C:\\Users\\Rober\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\metaestimators.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01meli5\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meli5\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PermutationImportance\n\u001b[0;32m      4\u001b[0m perm \u001b[38;5;241m=\u001b[39m PermutationImportance(model, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mfit(X_val, y_val)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\eli5\\__init__.py:13\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformatters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      7\u001b[0m     format_as_html,\n\u001b[0;32m      8\u001b[0m     format_html_styles,\n\u001b[0;32m      9\u001b[0m     format_as_text,\n\u001b[0;32m     10\u001b[0m     format_as_dict,\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexplain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m explain_weights, explain_prediction\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m explain_weights_sklearn, explain_prediction_sklearn\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transform_feature_names\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\eli5\\sklearn\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m absolute_import\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexplain_weights\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      4\u001b[0m     explain_weights_sklearn,\n\u001b[0;32m      5\u001b[0m     explain_linear_classifier_weights,\n\u001b[0;32m      6\u001b[0m     explain_linear_regressor_weights,\n\u001b[0;32m      7\u001b[0m     explain_rf_feature_importance,\n\u001b[0;32m      8\u001b[0m     explain_decision_tree,\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexplain_prediction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     11\u001b[0m     explain_prediction_sklearn,\n\u001b[0;32m     12\u001b[0m     explain_prediction_linear_classifier,\n\u001b[0;32m     13\u001b[0m     explain_prediction_linear_regressor,\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01munhashing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     InvertableHashingVectorizer,\n\u001b[0;32m     17\u001b[0m     FeatureUnhasher,\n\u001b[0;32m     18\u001b[0m     invert_hashing_and_fit,\n\u001b[0;32m     19\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\eli5\\sklearn\\explain_weights.py:78\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meli5\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transform_feature_names\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meli5\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_feature_importances\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     75\u001b[0m     get_feature_importances_filtered,\n\u001b[0;32m     76\u001b[0m     get_feature_importance_explanation,\n\u001b[0;32m     77\u001b[0m )\n\u001b[1;32m---> 78\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpermutation_importance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PermutationImportance\n\u001b[0;32m     81\u001b[0m LINEAR_CAVEATS \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124mCaveats:\u001b[39m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124m1. Be careful with features which are not\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124m   classification result for most examples.\u001b[39m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;241m.\u001b[39mlstrip()\n\u001b[0;32m     93\u001b[0m HASHING_CAVEATS \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;124mFeature names are restored from their hashes; this is not 100\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m precise\u001b[39m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;124mbecause collisions are possible. For known collisions possible feature names\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124mthe result is positive.\u001b[39m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;241m.\u001b[39mlstrip()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\eli5\\sklearn\\permutation_importance.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_cv\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetaestimators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m if_delegate_has_method\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_array, check_random_state\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     BaseEstimator,\n\u001b[0;32m     11\u001b[0m     MetaEstimatorMixin,\n\u001b[0;32m     12\u001b[0m     clone,\n\u001b[0;32m     13\u001b[0m     is_classifier\n\u001b[0;32m     14\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'if_delegate_has_method' from 'sklearn.utils.metaestimators' (C:\\Users\\Rober\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\metaestimators.py)"
     ]
    }
   ],
   "source": [
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "perm = PermutationImportance(model, random_state=0).fit(X_val, y_val)\n",
    "eli5.show_weights(perm, feature_names = val_X.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b209527",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877e29ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c conda-forge eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfb6736",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ede1cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
