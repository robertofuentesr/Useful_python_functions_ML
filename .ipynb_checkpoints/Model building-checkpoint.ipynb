{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9353abb4",
   "metadata": {},
   "source": [
    "ALl this notebook are a continuation on the last one in case it feels like some extra explanation is needed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "30c0b134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "# Read the data\n",
    "# This data you can find here: https://www.kaggle.com/c/home-data-for-ml-course/data\n",
    "\n",
    "X_full = pd.read_csv('train.csv', index_col='Id')\n",
    "X_test_full = pd.read_csv('test.csv', index_col='Id')\n",
    "\n",
    "# SalePrice is the target, if there is no target eliminate row associated with it\n",
    "X_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "y = X_full.SalePrice\n",
    "X = X_full.copy()\n",
    "X.drop(['SalePrice'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "X_test = X_test_full.copy()\n",
    "\n",
    "\n",
    "# Now we have X and y the target separate! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fc672b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_data(X,delete_over=10,col_to_change_to_null=0.1):\n",
    "    \n",
    "    categorical_variables = [col for col in  X.columns if str(X[col].dtypes)=='object']\n",
    "    #numerical_variables = [col for col in X.columns if str(X[col].dtypes)!='object']\n",
    "    cardinalidad = {}\n",
    "    for col in categorical_variables:\n",
    "        cardinalidad[col] = len(list(X[col].unique()))\n",
    "    # For now we delete categories with more values than..\n",
    "    delete_over = delete_over\n",
    "    columns_to_delete = [col for col in categorical_variables if len(list(X[col].unique()))>delete_over ]\n",
    "    X.drop(columns=columns_to_delete,inplace = True, axis=1)\n",
    "    \n",
    "    # We are going to change columns with too many null.\n",
    "    # We are not gonna delete them, will give them the chance to be important.\n",
    "    # that means that having or not having the value is what is really important.\n",
    "    col_to_change_to_null = col_to_change_to_null\n",
    "    columnas_modificar_por_1 = [col for col in X.columns if X[col].isnull().sum()>int(X.shape[0] * col_to_change_to_null) ]\n",
    "\n",
    "    for col in columnas_modificar_por_1:\n",
    "        X[col +str('_is_null')] = 0\n",
    "        X.loc[(X[col].isnull()),col +str('_is_null')] = 1\n",
    "\n",
    "    new_columns_null = [str(f\"{col}_is_null\") for col in columnas_modificar_por_1 ]    \n",
    "    X.drop(columns=columnas_modificar_por_1, axis=1,inplace=True)\n",
    "    \n",
    "    return X\n",
    "    \n",
    "def transforming_modeling_scoring(X,y,cv=5,n_jobs=16,n_iter=15, scoring='neg_mean_absolute_error'):\n",
    "    # separating the data in training/validation\n",
    "    # X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
    "    #                                                  random_state=0)        \n",
    "    \n",
    "    # We are gonna use a cross validation now\n",
    "    X_train = X\n",
    "    y_train = y\n",
    "    \n",
    "    numerical_col = [col for col in X_train.columns if str(X_train[col].dtypes)!='object' ]\n",
    "    numerical_col_imputed = [col for col in numerical_col if X_train[col].isnull().any()==True]\n",
    "\n",
    "    categorical_col = [col for col in X_train.columns if str(X_train[col].dtypes)=='object' ]\n",
    "    categorical_col_imputed = [col for col in categorical_col if X_train[col].isnull().any()==True]\n",
    "\n",
    "    numerical_transformer = SimpleImputer(strategy='mean')\n",
    "\n",
    "    categorical_transformer =  Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(missing_values=pd.NA, strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(transformers=\n",
    "        [(\"numerical_transformer\", numerical_transformer, numerical_col_imputed),\n",
    "        (\"categorical_transformer\", categorical_transformer, categorical_col)],remainder='passthrough')\n",
    "\n",
    "    # Define model\n",
    "    model = RandomForestRegressor(random_state=0,n_jobs=n_jobs)\n",
    "\n",
    "    # Bundle preprocessing and modeling code in a pipeline\n",
    "    pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                          ('model', model)\n",
    "                         ])\n",
    "\n",
    "    # pipe.fit(X_train,y_train)\n",
    "    \n",
    "    # scores = -1 * cross_val_score(pipe, X, y,cv=cv,scoring='neg_mean_absolute_error')\n",
    "    distributions = dict(model__n_estimators=randint(low=10,high=500))\n",
    "    clf = RandomizedSearchCV(pipe, distributions, random_state=0, cv=cv,n_iter=15,scoring=scoring)\n",
    "    search = clf.fit(X_train,y_train)\n",
    "    \n",
    "    return search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7f158e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cleaning_data(X)\n",
    "search = transforming_modeling_scoring(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9bcdf37b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__n_estimators': 369}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1b69ead4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17557.935208078106"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1*(search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cab7ced",
   "metadata": {},
   "source": [
    "Let us understand what we did above, we just use one function that uses cross validation to find the best number for n_estimators. It also gave us the best score achieve. \n",
    "What we are going to do below it is to test this, using cross validation ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9dcce11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforming_modeling_scoring(X,y,n_estimators=50,cv=5,n_jobs=16,n_iter=15, scoring='neg_mean_absolute_error'):\n",
    "    # separating the data in training/validation\n",
    "    # X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
    "    #                                                  random_state=0)        \n",
    "    \n",
    "    # We are gonna use a cross validation now\n",
    "    X_train = X\n",
    "    y_train = y\n",
    "    \n",
    "    numerical_col = [col for col in X_train.columns if str(X_train[col].dtypes)!='object' ]\n",
    "    numerical_col_imputed = [col for col in numerical_col if X_train[col].isnull().any()==True]\n",
    "\n",
    "    categorical_col = [col for col in X_train.columns if str(X_train[col].dtypes)=='object' ]\n",
    "    categorical_col_imputed = [col for col in categorical_col if X_train[col].isnull().any()==True]\n",
    "\n",
    "    numerical_transformer = SimpleImputer(strategy='mean')\n",
    "\n",
    "    categorical_transformer =  Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(missing_values=pd.NA, strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(transformers=\n",
    "        [(\"numerical_transformer\", numerical_transformer, numerical_col_imputed),\n",
    "        (\"categorical_transformer\", categorical_transformer, categorical_col)],remainder='passthrough')\n",
    "\n",
    "    # Define model\n",
    "    model = RandomForestRegressor(n_estimators=n_estimators,random_state=0,n_jobs=n_jobs)\n",
    "\n",
    "    # Bundle preprocessing and modeling code in a pipeline\n",
    "    pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                          ('model', model)\n",
    "                         ])\n",
    "\n",
    "    # pipe.fit(X_train,y_train)\n",
    "    \n",
    "    scores = -1 * cross_val_score(pipe, X, y,cv=cv,scoring=scoring)\n",
    "    # distributions = dict(model__n_estimators=randint(low=10,high=500))\n",
    "    # clf = RandomizedSearchCV(pipe, distributions, random_state=0, cv=cv,n_iter=15,scoring=scoring)\n",
    "    # search = clf.fit(X_train,y_train)\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bb7cd705",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cleaning_data(X)\n",
    "scores = transforming_modeling_scoring(X,y,n_estimators=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9c8b4ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17676.36709589041"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "de2937cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cleaning_data(X)\n",
    "scores = transforming_modeling_scoring(X,y,n_estimators=369)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4ee2b826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17557.935208078106"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6b5a5bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17599.18674129, 17502.66114452, 17475.31293388, 16098.70448639,\n",
       "       19113.81073431])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# notice there is quite a bit of variance\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ce2871",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
