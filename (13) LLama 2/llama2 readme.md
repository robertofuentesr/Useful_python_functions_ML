This is an example where I run a llama 2 example. Here is the original guide:
https://github.com/meta-llama/llama
What is interesting, is how it runs and what the output is. 
Here I ran the 7-B model and the results were impressive but still, the model made a mistake 
when I asked the meaning of a German word and invented the meaning,
so it can have hallucinations.

1) First the code in run_generation.txt must be run in the terminal those are the instruction of what code to run and it also passes the arguments to the functions.
2) Wait until the model runs, you can also check your task manager, I like to check if my code is using GPU.
3) Check the output of the model.
